{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Scripts_Task_3.ipynb","provenance":[],"collapsed_sections":["_SWLYCWsulE_","Yu614FNVzsic","zwhRTKh4xfJ-","yvWPCe_B1FdP","L6w-6pKRyTEk","ieGbwZQjvWIm","f9FBAPqhv1sT","qraZoDMYyXc_"],"authorship_tag":"ABX9TyPtuAJv9vBGfNz0UBaxh1gG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Task 3: Embeddings for primary and secondary structures"],"metadata":{"id":"nq6Z8LXQuUx1"}},{"cell_type":"markdown","source":["## Install required libraries and auxiliary functions"],"metadata":{"id":"aoezekX-ufDC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEAnL1AXuEfT"},"outputs":[],"source":["# We install the required libraries \n","# We use older versions of TF and numpy for compatibility\n","!pip install bert-tensorflow\n","!python3.7 -m pip install tensorflow-gpu==1.15.0\n","!pip install -U numpy==1.18.5"]},{"cell_type":"code","source":["import os\n","import numpy as np"],"metadata":{"id":"6Tw_J0SJuNcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We mount a google account's Drive to work in Google Colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"abtFrCIbuO-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Move to the dir where we have our code and clone the BERT \n","github repo so we can import modeling, tokenization and \n","optimization python files from it\n","'''\n","\n","os.chdir('/content/drive/MyDrive/Permed_Task_3/')"],"metadata":{"id":"OaREMc-4uP-8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Auxiliary functions to avoid embeddings higher than 1 \n","or lower than one and to add the required padding \n","if necessary\n","'''\n","def clip_long_seq(seq, max_len):\n","  if(len(seq) > max_len):\n","    seq = seq[0:max_len]\n","  return seq\n","def pad_sequence(seq,max_len):\n","  while(len(seq) < max_len):\n","    seq = seq + \"0\"\n","  return seq"],"metadata":{"id":"dJFurh61uTm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Primary structure"],"metadata":{"id":"_SWLYCWsulE_"}},{"cell_type":"markdown","source":["### Read the database and create windows of 30 aminoacids"],"metadata":{"id":"Yu614FNVzsic"}},{"cell_type":"code","source":["%cd Primary_structure"],"metadata":{"id":"9vtHBjlO88TT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Read the Database file and remove NewLine char(\\n)\n","'''\n","\n","entries = []\n","with  open('Database.txt') as fp:\n","    contents = fp.read()\n","    for entry in contents.split('-'):\n","      entry = entry.replace('\\n','')\n","      entries.append(entry)\n","entries.pop(0)"],"metadata":{"id":"7L_Ya1sPunJh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Create windows of 30 aminoacids\n","'''\n","n = 30\n","strings_30 = []\n","for entry in entries:\n","  split_strings = [entry[index : index + n] for index in range(0, len(entry), n)]\n","  '''for string in split_strings:\n","    if(len(string)<n):\n","      split_strings = split_strings[:-1]\n","      string = pad_sequence(string,n)\n","      split_strings.append(string)'''\n","\n","  strings_30.append(split_strings)"],"metadata":{"id":"4oNIKHgHw4u5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_write = open('database_30.txt','w')\n","for string in strings_30:\n","  for s in string:\n","    file_write.write(s + \"\\n\")"],"metadata":{"id":"deXnxANew7-4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Add spaces between tokens for the BERT model"],"metadata":{"id":"zwhRTKh4xfJ-"}},{"cell_type":"code","source":["'''\n","For the regular BERT model, we need a space between each aminoacid\n","This token is crucial for creating a training dataset and \n","for running BERT over the database\n","'''\n","database_file = 'database_30.txt'\n","file1 = open(database_file, 'r')\n","Lines = file1.readlines()\n","len(Lines)"],"metadata":{"id":"yZ86YItnw_dN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_lines = []\n","for line in Lines:\n","  line = (line.replace(\"\", \" \")[1: -1])\n","  final_lines.append(line)"],"metadata":{"id":"ehchsjNOxJlA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_write = open('database_30_spaces.txt','w')\n","for line in final_lines:\n","  file_write.write(line)"],"metadata":{"id":"bPfVTFFfxKlS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Obtaining feature vectors with TAPE library"],"metadata":{"id":"yvWPCe_B1FdP"}},{"cell_type":"code","source":["!pip install tape_proteins\n","#!pip install awscli --ignore-installed six"],"metadata":{"id":"7Dij33Xg1kWL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Install TAPe and create a model from pretrained\n","The TAPE tokenizer does not require spaces between \n","each aminoacid\n","'''\n","import torch\n","from tape import ProteinBertModel, TAPETokenizer\n","from tape import UniRepModel\n","import numpy as np\n","model = ProteinBertModel.from_pretrained('bert-base')\n","tokenizer = TAPETokenizer(vocab='iupac')"],"metadata":{"id":"E-dzDItH1nL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["database_file ='database_30.txt'\n","file1 = open(database_file,'r')\n","Lines = file1.readlines()\n","len(Lines)"],"metadata":{"id":"rQK1DTGI1qPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a counter variable is set to write the number of the subsequence\n","ctr = 0\n","file_write = open(\"output_primary.txt\",\"w\")\n","for s in Lines:\n","  # remove \\n character and clip sequences if it was necessary (we are using 30 as max_length so no required)\n","  s = s.replace('\\n','')\n","  s = clip_long_seq(s, 1024)\n","  # define the ids for the BERT model\n","  token_ids = torch.tensor([tokenizer.encode(s)])\n","  # create the output of the model\n","  output = model(token_ids)\n","  sq = torch.squeeze(output[0],dim=0)\n","  # output[0] is averaged to obtain the final embedding\n","  avg_output = torch.mean(sq,0)\n","  avg_output_corr = torch.clip(avg_output, min=-1,max=1)\n","  #print(torch.mean(avg_output_corr))\n","  file_write.write(str(ctr)+\"\\n\")\n","  file_write.write(str(avg_output_corr) + \"\\n\")\n","  ctr+=1\n","  #pooled output is not pretrained, so it is better to average input[0]\n","  #pooled_output = output[1]\n","  #file_write.write(str(avg_output_corr) + \"\\n\")"],"metadata":{"id":"nnCi1TZj1ryP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate pretraining data, training and running (if no TAPE)"],"metadata":{"id":"L6w-6pKRyTEk"}},{"cell_type":"code","source":["!python create_pretraining_data2.py --input_file=./database_30_spaces.txt\n","                                    --output_file=./tf_examples.tfrecord \n","                                    --vocab_file=./protein_vocab.txt\n","                                    --do_lower_case=True \n","                                    --max_seq_length=30 \n","                                    --max_predictions_per_seq=5\n","                                    --masked_lm_prob=0.15 \n","                                    --random_seed=12345 \n","                                    --dupe_factor=5"],"metadata":{"id":"3iiK11_JzAM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python run_pretraining.py --input_file=./tf_examples.tfrecord     \n","                            --output_dir=./outputs  \n","                            --do_train=True     \n","                            --do_eval=True     \n","                            --bert_config_file=./bert_config.json     \n","                            --train_batch_size=32     \n","                            --max_seq_length=30     \n","                            --max_predictions_per_seq=5     \n","                            --num_train_steps=14000000    \n","                            --num_warmup_steps=10000     \n","                            --learning_rate=1e-4     \n","                            --save_checkpoints_steps=10000"],"metadata":{"id":"rQ-hIbXm0ZRA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python extract_features4.py -–input_file=/home/pnunez/data/bert/sara/protein_subsequences.txt \n","                              -–output_file=/home/pnunez/data/bert/sara/output_primary.txt \n","                              -–vocab_file=/home/pnunez/data/bert/protein-vocab.txt \n","                              -–bert_config_file=/home/pnunez/data/bert/bert_config.json \n","                              -–init_checkpoint=/home/pnunez/data/bert/model2/model.ckpt-14000000 \n","                              --max_seq_length=30 \n","                              --layers=-1\n","                              -–batch_size=32  "],"metadata":{"id":"GOdETE4P07L1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Secondary structure"],"metadata":{"id":"AgLL2V_Cunqk"}},{"cell_type":"markdown","source":["### Read predictions file and remove unnecesary characters\n"],"metadata":{"id":"26d4EkwTu_GO"}},{"cell_type":"code","source":["%cd ../Secondary_structure"],"metadata":{"id":"48K6pnW49Okf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Read the prediction file from the U-NET and verify the\n","number of lines in the file\n","'''\n","\n","prediction_file = 'prediction.txt'\n","file1 = open(prediction_file, 'r')\n","Lines = file1.readlines()\n","final_lines=[]\n","for line in Lines:\n","  line=line.replace('\\n','')\n","  final_lines.append(line)"],"metadata":{"id":"dI2JjzIquowP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(Lines))\n","print(len(final_lines))"],"metadata":{"id":"Q0W-IWBIvMh1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Divide the predictions in windows of 30 and write a new file"],"metadata":{"id":"ieGbwZQjvWIm"}},{"cell_type":"code","source":["'''\n","This cell splits the previous lines in windows of 30 and adds \n","'''\n","\n","n = 30\n","strings_30 = []\n","chars = []\n","for line in final_lines:\n","  split_strings = [line[index : index + n] for index in range(0, len(line), n)]\n","  '''for string in split_strings:\n","    if(len(string)<n):\n","      split_strings = split_strings[:-1]\n","      string = pad_sequence(string,n)\n","      split_strings.append(string)'''\n","  '''for split in split_strings:\n","    for char in split:\n","      char = char + ' ' '''\n","\n","  strings_30.append(split_strings)"],"metadata":{"id":"P2oxJ-hgvJrH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_write = open('predictions_30.txt','w')\n","for string in strings_30:\n","  for s in string:\n","    file_write.write(s + \"\\n\")"],"metadata":{"id":"TzsIt4pGvTFV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Add spaces between tokens for the BERT model"],"metadata":{"id":"f9FBAPqhv1sT"}},{"cell_type":"code","source":["prediction_file = 'predictions_30.txt'\n","file1 = open(prediction_file, 'r')\n","Lines = file1.readlines()\n","len(Lines)"],"metadata":{"id":"VI0gBklZxWAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_lines = []\n","for line in Lines:\n","  line = (line.replace(\"\", \" \")[1: -1])\n","  final_lines.append(line)"],"metadata":{"id":"clsL6jChv5sI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_write = open('predictions_30_spaces.txt','w')\n","for line in final_lines:\n","  file_write.write(line)"],"metadata":{"id":"ci572qWRv7Eg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate pretraining data and training"],"metadata":{"id":"qraZoDMYyXc_"}},{"cell_type":"code","source":["!python create_pretraining_data2.py --input_file=./predictions_30_spaces.txt \n","                                    --output_file=./tf_examples.tfrecord \n","                                    --vocab_file=./secondary-vocab.txt \n","                                    --do_lower_case=True \n","                                    --max_seq_length=30 \n","                                    --max_predictions_per_seq=5 \n","                                    --masked_lm_prob=0.15 \n","                                    --random_seed=12345 \n","                                    --dupe_factor=5"],"metadata":{"id":"-ccFT9eYyaBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python run_pretraining.py --input_file=./tf_examples.tfrecord \n","                            --output_dir=./output \n","                            --do_train=True \n","                            --do_eval=True \n","                            --bert_config_file=./bert_config.json \n","                            --train_batch_size=32 \n","                            --max_seq_length=30 \n","                            --max_predictions_per_seq=5 \n","                            --num_train_steps=500000 \n","                            --num_warmup_steps=10000 \n","                            --learning_rate=1e-4 \n","                            --save_checkpoints_steps=10000"],"metadata":{"id":"JFyCQrANy0-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python extract_features4.py --input_file=predictions_30_spaces.txt \n","                              --output_file=./output/output_secondary.txt \n","                              --vocab_file=./secondary-vocab.txt \n","                              --bert_config_file=./bert_config.json \n","                              --init_checkpoint=model.ckpt-500000 \n","                              --max_seq_length=30 \n","                              --layers=-1 \n","                              --batch_size=32"],"metadata":{"id":"wBOUgRI2zfss"},"execution_count":null,"outputs":[]}]}